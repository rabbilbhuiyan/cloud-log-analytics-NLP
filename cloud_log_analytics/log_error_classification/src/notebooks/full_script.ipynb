{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fnmatch\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "# to ingnore the warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR= r'C:\\Users\\rbhuiyan\\Desktop\\log_classification\\log_error_classification\\data\\raw\\compute_nodes_5'\n",
    "PROCESSED_DATA_PATH = r'C:\\Users\\rbhuiyan\\Desktop\\log_classification\\log_error_classification\\data\\processed'\n",
    "\n",
    "\n",
    "\n",
    "# loading the data\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.BASE_DIR = BASE_DIR\n",
    "        \n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        self.new_list=[]\n",
    "        for root, dirs, files in os.walk(self.BASE_DIR):\n",
    "            for file in files:\n",
    "                if fnmatch.fnmatch(file, 'messages*'): #reading files that start with messages\n",
    "                    with open(os.path.join(root, file), 'r') as f:\n",
    "                        for line in f:\n",
    "                            if re.search('ERROR', line): #if 'ERROR' in line:\n",
    "                                self.new_list.append(line)\n",
    "        # converting list into string\n",
    "        new_string= ''.join(map(str, self.new_list))\n",
    "        df_log_error = pd.read_csv(StringIO(new_string), sep=\"\\n\", names=['Column'], engine='python')\n",
    "        # to view the column in wide\n",
    "        pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "        return df_log_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data wrangling : parsing the data and creating label \n",
    "\n",
    "class DataParser():\n",
    "\n",
    "    def __init__(self):\n",
    "        #self.input_dir = input_dir\n",
    "        #self.load_data()\n",
    "        self.PROCESSED_DATA_PATH = PROCESSED_DATA_PATH\n",
    "        self.parsed_data = None\n",
    "        #self.timestamp_data = None\n",
    "        self.parse_data()\n",
    "        self.create_label()\n",
    "        self.store_data()\n",
    "        #self.timestamp()\n",
    "        #self.create_label()\n",
    "        #pass\n",
    "    \n",
    "        \n",
    "\n",
    "    #Display and extract the features from the log data\n",
    "    def parse_data(self) ->pd.DataFrame:\n",
    "        '''\n",
    "        data parsing.\n",
    "        Args:\n",
    "            df {pandas.DataFrame}: dataset\n",
    "        Return:\n",
    "            pandas.Dataframe: updated data with new features\n",
    "        '''\n",
    "        \n",
    "        # call the instance of the class DataLoader\n",
    "        parsed_data = DataLoader().load_data()\n",
    "        \n",
    "        # spliting the data into variables\n",
    "        parsed_data[['timestamp', 'server','component','date', 'time', 'customer_ID','event', 'nova_compute', 'log_message']] = parsed_data['Column'].str.split(' ', 8, expand=True)\n",
    "        \n",
    "        # sub dataframe\n",
    "        parsed_data = parsed_data[['server','component','date', 'time', 'customer_ID', 'nova_compute', 'log_message']]\n",
    "        \n",
    "        # extracting the info within the square bracket\n",
    "        parsed_data['request_ID'] = parsed_data['log_message'].str.extract('\\[(.*?)\\]', expand=False).str.strip()\n",
    "        \n",
    "        # removing the square bracket and the contents within the square bracket\n",
    "        parsed_data['log_message'] = parsed_data['log_message'].str.replace(r'\\[.*?\\]','')\n",
    "\n",
    "        # Taking only first 100 characthers of the log message\n",
    "        parsed_data['log_message'] = parsed_data['log_message'].str[:100]\n",
    "\n",
    "        # stripping off everything after #\n",
    "        parsed_data['log_message'] = parsed_data['log_message'].str.split('#').str[0]\n",
    "        \n",
    "        ##########################\n",
    "        # deleting the non-date values: first make the non-dates to NaT value, then apply dropna() method\n",
    "        parsed_data['date'] = pd.to_datetime(parsed_data['date'], errors='coerce')\n",
    "        parsed_data = parsed_data.dropna(subset=['date'])\n",
    "\n",
    "        # creating a timestamp column\n",
    "        date_time=pd.to_datetime(parsed_data['date'].astype(str)+ ' '+ parsed_data['time'].astype(str))\n",
    "        parsed_data.insert(0, 'timestamp', date_time)\n",
    "\n",
    "        # droping the columns date and time\n",
    "        parsed_data.drop(['date','time'], axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        return parsed_data\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def create_label(self) ->pd.DataFrame:\n",
    "    \n",
    "        labeled_data = DataParser.parse_data(self)\n",
    "        \n",
    "        conditions = [\n",
    "            (labeled_data['log_message'].str.contains('service status')),\n",
    "            #(df3_error['log_message_3'].str.contains('network cache')) & (df3_error['log_message_3'].str.contains('ConnectTimeout')),\n",
    "            (labeled_data['log_message'].str.contains('network cache')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager.update')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager._heal')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager._sync_scheduler')),\n",
    "            (labeled_data['log_message'].str.contains('ConnectTimeout')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager._run_pending')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager._cleanup_incomplete')),\n",
    "            (labeled_data['log_message'].str.contains('connection blocked')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager._sync_power')),\n",
    "            (labeled_data['log_message'].str.contains('AMQP server on pouta2')),\n",
    "            (labeled_data['log_message'].str.contains('AMQP server on pouta1')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager._run_image')),\n",
    "            (labeled_data['log_message'].str.contains('ComputeManager._cleanup_running')),\n",
    "            (labeled_data['log_message'].str.contains('Error updating resources for node')),\n",
    "            (labeled_data['log_message'].str.contains('Unable to access floating IP')),\n",
    "            (labeled_data['log_message'].str.contains('ProcessExecutionError')),\n",
    "            (labeled_data['log_message'].str.contains('InvalidSharedStorage_Remote')),\n",
    "            (labeled_data['log_message'].str.contains('Failed storing info cache'))]\n",
    "\n",
    "        # create a list of the values we want to assign for each condition\n",
    "        values = ['service status', 'network cache', 'CP update resource', 'CP heal instance', 'CP sync scheduler',\n",
    "                 'NC ConnectTimeout', 'CP run pending', 'CP clearup migrations', 'broker blocked connection','CP sync power',\n",
    "                 'AMQP server pouta2',  'AMQP server pouta1','CP run image cache','CP cleanup running instance','node updating',\n",
    "                 'floating IP access','ProcessExecutionError','InvalidSharedStorage_Remote', 'storing info cache']\n",
    "\n",
    "        # create a new column and use np.select to assign values to it using the lists as arguments\n",
    "        labeled_data['label'] = np.select(conditions, values)\n",
    "        \n",
    "        return labeled_data\n",
    "\n",
    "\n",
    "\n",
    "    def store_data(self) ->pd.DataFrame:\n",
    "        #self.parsed_data.to_csv(PROCESSED_DATA_PATH, index = False, header = True)\n",
    "        # csv the file so we do not need to reprocess each time\n",
    "        self.csv_processed_df_filename = 'processed_data.csv'\n",
    "        self.csv_file_loc = os.path.join(PROCESSED_DATA_PATH, self.csv_processed_df_filename)\n",
    "        data_saved = DataParser.create_label(self)\n",
    "        # df to csv\n",
    "        data_saved.to_csv(self.csv_file_loc, index = False, header = True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def sub_dataframe(self) ->pd.DataFrame:\n",
    "        final_dataframe = DataParser().create_label()\n",
    "        final_dataframe = final_dataframe[['log_message', 'label']]\n",
    "        \n",
    "        return final_dataframe\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "class DataPreprocess():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw_text = None\n",
    "        \n",
    "\n",
    "        #print(self.labeled_data.head(2))\n",
    "        \n",
    "    # fucntion to clean the text\n",
    "    def clean_text(raw_text):\n",
    "        \n",
    "    \n",
    "        # keep only words\n",
    "        letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "        # convert to lower case and split\n",
    "        words = letters_only_text.lower().split()\n",
    "\n",
    "        # remove the single word\n",
    "        no_single_words = [word for word in words if len(word)>1]\n",
    "        # in case of dataframe\n",
    "        #df[''].map(lambda x: ' '.join(word for word in x.split() if len(word)>1))\n",
    "\n",
    "        # remove stopwords\n",
    "        stopwords_set = set(stopwords.words(\"english\"))\n",
    "        meaningful_words = [w for w in no_single_words if w not in stopwords_set]\n",
    "\n",
    "        # stemmed words\n",
    "        #ps = PorterStemmer()\n",
    "        #stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
    "\n",
    "        # tokenize the words\n",
    "        #tokenized_words = [word_tokenize(entry) for entry in meaningful_words]\n",
    "\n",
    "\n",
    "        # join the cleanned words in a list\n",
    "        cleaned_word_list = \" \".join(meaningful_words)\n",
    "\n",
    "        return cleaned_word_list\n",
    "    \n",
    "    \n",
    "    def clean_log(self) ->pd.DataFrame:\n",
    "        cleaned_log = DataParser().sub_dataframe()\n",
    "        #aa= df1['log_message'].apply(DataPreprocess)\n",
    "        #bb= df1['log_message'].apply(lambda x: DataPreprocess(x))\n",
    "        cleaned_log['processed_log']= cleaned_log['log_message'].apply(lambda x: DataPreprocess.clean_text(x))\n",
    "        cleaned_log.drop(['log_message'], axis=1, inplace=True)\n",
    "        \n",
    "        return cleaned_log\n",
    "    \n",
    "    \n",
    "    # transform the label categories into distinct integer values representing the initial categorical values\n",
    "    def create_label_encoder(self) ->pd.DataFrame:\n",
    "        \n",
    "        encoded_label = DataPreprocess().clean_log()\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        encoded_label['label_ID'] = label_encoder.fit_transform(encoded_label['label'])\n",
    "        \n",
    "        # Also its good to have the categories as a dictionary\n",
    "        label_map = encoded_label.set_index('label_ID').to_dict()['label']\n",
    "        #print(label_map)\n",
    "\n",
    "        return encoded_label\n",
    "        \n",
    "        \n",
    "\n",
    "# Train the data into classifier\n",
    "\n",
    "class TrainModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.classifier = None\n",
    "        self.vectorizer = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data():\n",
    "        \n",
    "        data = DataPreprocess().create_label_encoder()\n",
    "        X = data['processed_log']\n",
    "        y = data['label_ID']\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "    # creating function for evaluation \n",
    "\n",
    "    @staticmethod # alternatively we can add 'self' as an argument for the object\n",
    "    def evaluate_classifier(title, classifier, vectorizer, X_test, y_test):\n",
    "        \n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "        precision = metrics.precision_score(y_test, y_pred, average= 'weighted')\n",
    "        recall = metrics.recall_score(y_test, y_pred, average= 'weighted')\n",
    "        f1 = metrics.f1_score(y_test, y_pred, average= 'weighted')\n",
    "\n",
    "        print(\"%s\\t%f\\t%f\\t%f\\n\" % (title, precision, recall, f1))\n",
    "\n",
    "    \n",
    "    # creating the training classifier\n",
    "\n",
    "    def train_classifier(data):\n",
    "        \n",
    "        # load data\n",
    "        X, y = TrainModel().load_data()\n",
    "        \n",
    "        # splitting the data into train test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "        # the object to turn data (text) into vectors\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        # create doceument term matrix\n",
    "        dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "        # train Naive Bayes classifier\n",
    "        naive_bayes_classifier = MultinomialNB().fit(dtm, y_train)\n",
    "\n",
    "        # evaluating the model accuracy  \n",
    "        TrainModel().evaluate_classifier('Naive Bayes\\tTRAIN\\t', naive_bayes_classifier, vectorizer, X_train, y_train)\n",
    "\n",
    "        TrainModel().evaluate_classifier('Naive Bayes\\tTEST\\t', naive_bayes_classifier, vectorizer, X_test, y_test)\n",
    "\n",
    "        # store the classifier so we can call that\n",
    "        clf_filename = 'naive_bayes_classifier.pkl'\n",
    "        pickle.dump(naive_bayes_classifier, open(clf_filename, 'wb'))\n",
    "\n",
    "        # store the vectorizer so we can transform to new data\n",
    "        vec_filename = 'tfidf_vectorizer.pkl'\n",
    "        pickle.dump(vectorizer, open(vec_filename, 'wb'))\n",
    "        \n",
    "        return naive_bayes_classifier\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Predict the model\n",
    "\n",
    "class PredictModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.cm = None\n",
    "        self.classes = None\n",
    "    \n",
    "   \n",
    "    # function to print and plot the confusion matrix\n",
    "    \n",
    "    @staticmethod # adding this TypeError: plot_confusion_matrix() got multiple values for argument 'classes' is solved\n",
    "    def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "        classes: given classification classes such as [0, 1, 2]\n",
    "                      the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "        normalize:    If False, plot the raw numbers\n",
    "                      If True, plot the proportions\n",
    "        \n",
    "        title:        the text to display at the top of the matrix\n",
    "\n",
    "        cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                      see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                      plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "        Usage\n",
    "        -----\n",
    "        plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                                  # sklearn.metrics.confusion_matrix\n",
    "                              normalize    = True,                # show proportions\n",
    "                              target_names = y_labels_vals,       # list of names of the classes\n",
    "                              title        = best_estimator_name) # title of graph\n",
    "\n",
    "        Citiation\n",
    "        ---------\n",
    "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "        \"\"\"\n",
    "        import itertools\n",
    "        import numpy as np\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "        print(cm)\n",
    "        \n",
    "        accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "        misclass = 1 - accuracy\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45) \n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    \n",
    "        \n",
    "    # creating prediction classifier\n",
    "    \n",
    "    def predict(self):\n",
    "        \n",
    "        # load data\n",
    "        X, y = TrainModel().load_data()\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42 )\n",
    "        \n",
    "        # the object to turn data (text) into vectors\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        # create doceument term matrix\n",
    "        dtm = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "        model = TrainModel().train_classifier()\n",
    "        \n",
    "        y_predicted = model.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        \n",
    "        print('accuracy %s' % accuracy_score(y_predicted, y_test))\n",
    "        print(classification_report(y_test, y_predicted)) # target_names=my_label\n",
    "        \n",
    "        \n",
    "        #########################\n",
    "        cnf_matrix = confusion_matrix(y_test, y_predicted)\n",
    "        np.set_printoptions(precision=2)\n",
    "\n",
    "        # Plot non-normalized confusion matrix\n",
    "        plt.figure(figsize=(12,10))\n",
    "        PredictModel().plot_confusion_matrix(cnf_matrix, classes=['network cache', 'service status', 'CP heal instance',\n",
    "       'CP update resource', 'CP run pending', 'CP sync scheduler',\n",
    "       'CP clearup migrations', 'CP run image cache', 'CP sync power',\n",
    "       'broker blocked connection', 'AMQP server pouta1', 'node updating',\n",
    "       'InvalidSharedStorage_Remote', 'ProcessExecutionError',\n",
    "       'CP cleanup running instance', 'AMQP server pouta2',\n",
    "       'floating IP access', 'storing info cache'], title='Confusion matrix, without normalization')\n",
    "        \n",
    "        # saving the plot\n",
    "        plt.savefig('confusion_matrix_plot')\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    #@staticmethod- we can use the @staticmethod decorator to avoid the error, If the method doesn't require self as an argument\n",
    "    def classify(self, data):\n",
    "\n",
    "        # load classifier\n",
    "        clf_filename = 'naive_bayes_classifier.pkl'\n",
    "        nb_clf = pickle.load(open(clf_filename, 'rb'))\n",
    "\n",
    "        # vectorize the new text\n",
    "        vec_filename = 'tfidf_vectorizer.pkl'\n",
    "        vectorizer = pickle.load(open(vec_filename, 'rb'))\n",
    "\n",
    "        pred = nb_clf.predict(vectorizer.transform([data]))\n",
    "\n",
    "        print(pred[0])\n",
    "        #print('accuracy %s' % accuracy_score(pred, y_test))\n",
    "        #return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# printing the output  \n",
    "\n",
    "predictModelInstance = PredictModel()\n",
    "#predict_result = predictModelInstance.predict()\n",
    "#print(predict_result)\n",
    "\n",
    "\n",
    "# deployment in production\n",
    "\n",
    "new_data =\"AMQP server on pouta1:5672 is unreachable: timed out. Trying again in 1 seconds\"\n",
    "classify_data = predictModelInstance.classify(new_data)\n",
    "print(classify_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272788c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,../scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
